\section{Testing Methodology}
\game{} was tested continuously throughout development. This was primarily done using a single machine with multiple instances of the game running as separate processes, each of which communicated via a single NFD. This was an ideal setup for design, development and debugging. However, it is not representative of a real scenario in which multiple players are playing \game{}. As such, a more realistic approach was required. 

A description of the libraries and frameworks used, and the software developed to facilitate real-world testing of \game{} is provided in the following sections.



\subsection{Player Automation}\label{sec:impl:automation}
In order to enable testing without actual people playing the game, a simple automation mechanism was developed. The automation script used caused players to move in approximate paths, while shooting projectiles approximately once per second and placing blocks approximately once every 10 seconds. A random number generator was used to ensure that a player would not just repeatedly loop through the same move-set. However, a fixed seed was used for the random number generator, providing repeatability across tests using different topologies and parameters, enabling direct comparisons between these tests. 


\subsection{Docker}\label{sec:impl:test:docker}
As previously discussed, having several Java processes communicating using a single NFD does not represent a realistic scenario. Thus, an approach using a separate NFD for each player was developed, which closely resembles the real-world use case of multiple players playing on different machines connected by a network.

To accomplish this, Docker was used. A Docker image which contains everything required to run \game{} was developed based on Peter Gusev's \cite{docker-ndn} NDN Docker image. This enabled \game{} to be run on any machine which had Docker installed. For each of the topologies tested, a separate \textit{Docker compose file} was created, which contained a \textit{service} for each node in the topology. Thus, each automated game player ran as a \textit{service}, as did each intermediate router.

In order to test the scalability of \game{}, dozens of containers needed to be run simultaneously. However, this approach did not scale well on a single machine, and an approach was required to enable deploying the Docker containers across a cluster. Docker also allows for the creation of virtual networks, over which Docker containers can communicate. An overlay network was created, enabling each node in the Docker swarm to communicate. This was done by giving each of the nodes a \textit{network alias}. For example, node A was given the alias \textit{nodea.ndngame.com} and node B was given the alias \textit{nodeb.ndngame.com} etc. Docker Compose supports defining network aliases in the service definition, and this is the reason each node was defined as a separate service.

This provided a mechanism for any two nodes to communicate using a UDP tunnel through the overlay network created by Docker. However, a key component of the testing is defining the NDN topology and seeing how it impacts the performance of \game{}. Even though every node can directly access every other node using a UDP tunnel, the actual paths taken by the NDN Interest / Data packets are defined by the FIB of the NFDs of each node. 

NLSR \cite{nlsr} is the routing protocol used by NDN to discover nearby nodes (routers) and to build the FIB. The NLSR daemon requires a configuration file which defines a variety of NLSR specific parameters, as well as the name of the node, adjacent nodes and the name prefixes which this node can produce data under. 

By creating a configuration file for each node in the NDN topology, and running the NLSR daemon on each of the test nodes with the appropriate configuration file, the NDN topology could be fully defined, and all players will discover the required prefixes and their NFDs will function appropriately.

To examine the impacts of the topology on the performance of \game{}, a Python script was developed to perform the following:

\begin{enumerate}
    \item Allow for topologies to be defined in code, supporting both game players and intermediate routers.
    \item Generate appropriate NLSR config files for each node in the topology
    \item Generate an appropriate Docker Compose file for the topology, allowing all nodes in the topology to communicate and for the entire test to be runnable with a single command.
\end{enumerate}

The script was then used to generate the required files for testing a variety of topologies such as the ones shown in \reffig{fig:impl:topologies}.


\begin{figure}
    \centering
    \figsize{assets/impl/topologies.png}{0.5}
    \caption{An example of the topologies used for testing \game{}}
    \label{fig:impl:topologies}
\end{figure}

These topologies were chosen as they offer a good coverage of possible topologies. The \textit{linear} and \textit{square} topologies represent two and four player nodes respectively without any intermediate routers. The \textit{tree} and \textit{dumbbell} topologies model hierarchical topologies, such as what would be found with nodes A and B connected to one Internet service provider (ISP), and nodes C and D connected to another.  


\section{Metrics}\label{sec:impl:metrics}
In order to examine the performance of \game{}, several metrics were required. There were two sources of data for these metrics: the \game{} processes running on the player nodes and the NFD logs running on the player nodes and intermediate router nodes. 

To gather data from the \game{} Java processes, Dropwizard's Metrics framework \cite{dropwizard-metrics} was used. Defining metrics proved to be considerably more challenging than expected due to nature of NDN. The main issue is that Interest packets are aggregated and Data packets are multicasted to consumers. This means there is no way for producers to know which consumer they are serving. Similarly, consumers cannot know if they are obtaining a cached copy of a piece of Data, or if the piece of Data they receive is the result of another player expressing the same Interest at an earlier point in time. Thus, a brief description of the metrics used is given below.

\subsection{Round-Trip-Time (RTT)}
One of the main challenges was attempting to understand the temporal performance of the network. Recall that the sync protocol uses a long-lived outstanding Interest model, and that producers only reply when there is an update worth sending. Thus, if a consumer expresses an Interest at time $t_{int}$, and receives a Data packet at time $t_{data}$, the RTT can be calculated using \refeq{eq:impl:rtt},

\begin{equation}\label{eq:impl:rtt}
    RTT = t_{data} - t_{int}
\end{equation}

Consider the case where a consumer calculates a RTT of 500ms. This would suggest that the network is performing very poorly. However, this may be due to the fact that the producer had no updates to send when the Interest arrived,  meaning the Interest took considerably longer to satisfy, even though the network may be performing well.

The obvious solution to this problem is to have producers respond with the amount of time that elapsed between receipt of the Interest, and an update becoming available, $t_{wait}$. This would allow the RTT calculation to be adjusted, yielding the \textit{effective RTT, $RTT_{eff}$} as shown in \refeq{eq:impl:rtt-eff}.

\begin{equation}\label{eq:impl:rtt-eff}
    RTT_{eff} = (t_{data} - t_{int}) - t_{wait}
\end{equation}

However, due to Interest aggregation, the producer will only see one Interest for a given Data name, regardless of the number of consumers who express the same Interest. Thus, the producer can only measure $t_{wait}$ for the first Interest they receive. Similarly, the Data packet received by all of the consumers will be identical due to the native multicast feature of NDN. Thus, $t_{wait}$ will only be correct for the consumer whose Interest reached the producer first.

This problem does not exist in a typical IP implementation as the producer would receive a request from each consumer and thus be able to calculate $t_{wait}$ accordingly.

For this reason, the first RTT metric $RTT$ was used instead of $RTT_{eff}$ and this effect was taken into account when evaluating RTTs.


\subsection{Interest Aggregation Factor (IAF)}
As previously discussed, one of the major benefits of NDN in a P2P MOG context such as \game{} is Interest aggregation. The \textit{interest aggregation factor (IAF)} is a measure of how much Interest aggregation is occurring in the network. The IAF of a particular producer is the ratio of the number of Interests seen by the producer to the number of Interests expressed by consumers for Data owned by that producer.

The \textit{IAF} for a player $p$, $iaf_p$, can be calculated using \refeq{eq:impl:iaf}, where $P$ represents the set of all players, $n_{seen\_p}$ represents the number of Interests seen by player $p$ and $n_{exp\_x\_p}$ represents the number of Interests expressed by player \textit{x} for Data produced by player \textit{p}.

\begin{equation}\label{eq:impl:iaf}
    iaf_p = \frac{n_{seen\_p}}{\sum\limits_{x \epsilon P}^{}{n_{exp\_x\_p}}}
\end{equation}


\subsection{Cache Hit Rate}
As there is no way for consumers to know if they received a cached copy of a Data packet, the easiest way to accurately determine cache rates was to enable debug mode logging in the NFD's \textit{ContentStore} module. This produces log messages indicating whether or not an entry existed in a CS. A Python script was then written to parse the log files of all the participating nodes to determine their cache hit rates.

A more elegant alternative would be to add this functionality to the NFD used by all of the nodes. However, the amount of log messages produced by enabling logging of the \textit{ContentStore} module was sufficiently small that the simpler approach sufficed.

\subsection{Position Deltas}
In order to investigate the performance of the dead reckoning (DR) and Interest management (IM) features, a metric was required which captured the error associated with enabling these features and adjusting their parameters.

The \textit{position delta} metric is calculated on receipt of an update for a game object. It is simply the Euclidean distance between the position of the local copy of the remote game object, and the position contained in the received update. 

The network benefits of DR and IM can be examined using other metrics, but the more aggressively these systems are used, the worse the gameplay experience becomes. However, as the test players are automated and run in headless mode, the \textit{position delta} was used to attempt to numerically measure the associated impact on gameplay experience.


